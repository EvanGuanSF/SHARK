model_name, use_tracing, model_type, dynamic, param_count, tags, notes
bert-large-uncased_training,True,Training,False,336M,"nlp;bert-bariant;transformer-encoder","24 layers, 1024 hidden; 16 attention heads"
